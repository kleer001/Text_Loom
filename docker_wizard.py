#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
from pathlib import Path
from typing import Dict, List, Tuple
from enum import Enum

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

try:
    from core.findLLM import load_config
    FINDLLM_AVAILABLE = True
except ImportError:
    FINDLLM_AVAILABLE = False


class Style(Enum):
    HEADER = '\033[95m'
    INFO = '\033[94m'
    SUCCESS = '\033[92m'
    WARNING = '\033[93m'
    ERROR = '\033[91m'
    RESET = '\033[0m'
    BOLD = '\033[1m'


class CloudProvider:
    def __init__(self, name: str, key_name: str, default_model: str):
        self.name = name
        self.key_name = key_name
        self.default_model = default_model


CLOUD_PROVIDERS = {
    1: CloudProvider("ChatGPT", "OPENAI_API_KEY", "gpt-4"),
    2: CloudProvider("Claude", "ANTHROPIC_API_KEY", "claude-3-5-sonnet-20240620"),
    3: CloudProvider("Perplexity", "PERPLEXITY_API_KEY", "pplx-7b-chat"),
    4: CloudProvider("Gemini", "GOOGLE_API_KEY", "gemini-1.5-pro"),
}

SEPARATOR_WIDTH = 60
DEFAULT_MODEL = "llama3:latest"


def styled_print(text: str, style: Style = Style.INFO, prefix: str = ""):
    print(f"{style.value}{prefix}{text}{Style.RESET.value}")


def print_separator():
    styled_print("=" * SEPARATOR_WIDTH, Style.BOLD)


def print_header(text: str):
    print()
    print_separator()
    styled_print(text.center(SEPARATOR_WIDTH), Style.BOLD)
    print_separator()
    print()


def prompt_choice(question: str, options: List[str]) -> int:
    styled_print(f"\n{question}", Style.BOLD)
    for i, option in enumerate(options, 1):
        print(f"  {i}. {option}")

    while True:
        try:
            choice = int(input(f"\n{Style.BOLD.value}Choice: {Style.RESET.value}").strip())
            if 1 <= choice <= len(options):
                return choice
            styled_print(f"Enter 1-{len(options)}", Style.ERROR, "✗ ")
        except ValueError:
            styled_print("Enter a valid number", Style.ERROR, "✗ ")
        except KeyboardInterrupt:
            sys.exit(0)


def prompt_yes_no(question: str, default: bool = True) -> bool:
    prompt = f"{question} [{'Y/n' if default else 'y/N'}]: "
    while True:
        try:
            response = input(f"{Style.BOLD.value}{prompt}{Style.RESET.value}").strip().lower()
            if not response:
                return default
            if response in ['y', 'yes']:
                return True
            if response in ['n', 'no']:
                return False
            styled_print("Enter 'y' or 'n'", Style.ERROR, "✗ ")
        except KeyboardInterrupt:
            sys.exit(0)


def prompt_text(question: str, default: str = "", required: bool = False) -> str:
    prompt = f"{question}{f' [{default}]' if default else ''}: "
    while True:
        try:
            response = input(f"{Style.BOLD.value}{prompt}{Style.RESET.value}").strip()
            if not response and default:
                return default
            if response or not required:
                return response
            styled_print("This field is required", Style.ERROR, "✗ ")
        except KeyboardInterrupt:
            sys.exit(0)


def check_prerequisites() -> Tuple[bool, bool]:
    docker_ok = shutil.which("docker") is not None
    compose_ok = shutil.which("docker-compose") is not None or shutil.which("docker")
    return docker_ok, compose_ok


def detect_llm_platforms() -> List[str]:
    if not FINDLLM_AVAILABLE:
        return []

    try:
        config = load_config()
        return [s for s in config.sections() if s != 'DEFAULT']
    except Exception:
        return []


def load_existing_env() -> Dict[str, str]:
    env_vars = {}
    if not os.path.exists(".env"):
        return env_vars

    with open(".env", "r") as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith("#") and "=" in line:
                key, value = line.split("=", 1)
                env_vars[key.strip()] = value.strip()
    return env_vars


def save_env(env_vars: Dict[str, str]):
    with open(".env", "w") as f:
        f.write("# Text Loom Docker Environment Configuration\n")
        f.write("# Generated by docker_wizard.py\n\n")
        for key, value in env_vars.items():
            f.write(f"{key}={value}\n")


def select_llm_from_list(detected: List[str]) -> str:
    styled_print(f"\nDetected {len(detected)} LLM platform(s):", Style.SUCCESS, "✓ ")
    for llm in detected:
        print(f"  • {llm}")

    if len(detected) == 1:
        if prompt_yes_no(f"\nUse {detected[0]}?"):
            return detected[0]
        return prompt_text("Enter LLM platform name", "Ollama")

    choice = prompt_choice("Select LLM platform:", detected + ["Other"])
    return detected[choice - 1] if choice <= len(detected) else prompt_text("Enter LLM platform name", "Ollama")


def configure_local_llm(detected: List[str]) -> Dict[str, str]:
    styled_print("\nDocker will use 'host.docker.internal' to access host", Style.INFO, "ℹ ")

    llm_name = select_llm_from_list(detected) if detected else prompt_text("Enter LLM platform name", "Ollama")
    model = prompt_text("Enter model name", DEFAULT_MODEL)

    styled_print("\nUpdate src/core/settings.cfg:", Style.WARNING, "⚠ ")
    styled_print("  Change 'localhost' to 'host.docker.internal'", Style.INFO, "ℹ ")

    return {"ACTIVE_LLM": llm_name, "LLM_MODEL": model}


def configure_ollama_docker() -> Dict[str, str]:
    model = prompt_text("Enter model to use", DEFAULT_MODEL)
    styled_print("\nPull model after starting:", Style.INFO, "ℹ ")
    print(f"  docker exec -it textloom-ollama ollama pull {model}")

    return {
        "ACTIVE_LLM": "Ollama",
        "LLM_MODEL": model,
        "OLLAMA_IN_DOCKER": "true"
    }


def configure_cloud_llm() -> Dict[str, str]:
    provider_names = [p.name for p in CLOUD_PROVIDERS.values()]
    choice = prompt_choice("Select cloud provider:", provider_names)

    provider = CLOUD_PROVIDERS[choice]
    api_key = prompt_text(f"Enter {provider.name} API key", required=True)
    model = prompt_text("Enter model name", provider.default_model)

    return {
        "ACTIVE_LLM": provider.name,
        provider.key_name: api_key,
        "LLM_MODEL": model
    }


def configure_environment(detected: List[str]) -> Dict[str, str]:
    print_header("Environment Configuration")

    if os.path.exists(".env") and not prompt_yes_no("Overwrite existing .env?", False):
        styled_print("Using existing .env", Style.INFO, "ℹ ")
        return load_existing_env()

    location = prompt_choice(
        "Where will your LLM run?",
        [
            "Local on host (already running)",
            "Ollama in Docker container",
            "Cloud API (OpenAI, Claude, etc.)",
            "Skip (configure later)"
        ]
    )

    configs = {
        1: lambda: configure_local_llm(detected),
        2: configure_ollama_docker,
        3: configure_cloud_llm,
        4: lambda: {}
    }

    env_vars = configs[location]()

    if env_vars:
        save_env(env_vars)
        styled_print(".env created successfully", Style.SUCCESS, "✓ ")

    return env_vars


def enable_ollama_service():
    styled_print("Enabling Ollama service...", Style.INFO, "ℹ ")

    with open("docker-compose.yml", "r") as f:
        content = f.read()

    replacements = [
        ("# ollama:", "ollama:"),
        ("#   image: ollama/ollama", "  image: ollama/ollama"),
        ("#   container_name:", "  container_name:"),
        ("#   ports:", "  ports:"),
        ('#     - "11434:11434"', '    - "11434:11434"'),
        ("#   volumes:", "  volumes:"),
        ("#     - ollama-data:", "    - ollama-data:"),
        ("#   restart:", "  restart:"),
        ("# ollama-data:", "ollama-data:"),
    ]

    for old, new in replacements:
        content = content.replace(old, new)

    with open("docker-compose.yml", "w") as f:
        f.write(content)

    styled_print("docker-compose.yml updated", Style.SUCCESS, "✓ ")


def launch_containers(use_ollama: bool):
    print_header("Starting Docker Services")
    styled_print("Building and starting containers...", Style.INFO, "ℹ ")

    result = subprocess.run(
        ["docker-compose", "up", "-d", "--build"],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        styled_print("Failed to start containers", Style.ERROR, "✗ ")
        print(result.stderr)
        return False

    styled_print("Containers started successfully!", Style.SUCCESS, "✓ ")

    if use_ollama and prompt_yes_no("\nPull Ollama model now?"):
        model = prompt_text("Model name to pull", "llama3")
        styled_print(f"Pulling {model}...", Style.INFO, "ℹ ")
        subprocess.run(["docker", "exec", "textloom-ollama", "ollama", "pull", model])

    print(f"\n{'='*SEPARATOR_WIDTH}")
    styled_print("Text Loom is running!", Style.SUCCESS, "✓ ")
    styled_print("API: http://localhost:8000", Style.INFO, "ℹ ")
    styled_print("Docs: http://localhost:8000/api/v1/docs", Style.INFO, "ℹ ")
    print(f"{'='*SEPARATOR_WIDTH}\n")

    styled_print("Useful commands:", Style.INFO, "ℹ ")
    print("  • docker-compose logs -f")
    print("  • docker-compose down")
    print("  • docker-compose restart")

    return True


def main():
    print_header("Text Loom Docker Setup Wizard")

    os.chdir(Path(__file__).parent)

    print_header("Step 1: Prerequisites")
    docker_ok, compose_ok = check_prerequisites()

    if not docker_ok:
        styled_print("Docker not found", Style.ERROR, "✗ ")
        styled_print("Install: https://docs.docker.com/get-docker/", Style.INFO, "ℹ ")
        sys.exit(1)

    if not compose_ok:
        styled_print("Docker Compose not found", Style.ERROR, "✗ ")
        styled_print("Install: https://docs.docker.com/compose/install/", Style.INFO, "ℹ ")
        sys.exit(1)

    styled_print("Docker installed", Style.SUCCESS, "✓ ")
    styled_print("Docker Compose installed", Style.SUCCESS, "✓ ")

    print_header("Step 2: LLM Detection")
    styled_print("Scanning src/core/settings.cfg...", Style.INFO, "ℹ ")

    detected = detect_llm_platforms()
    if detected:
        styled_print(f"Found {len(detected)} platform(s):", Style.SUCCESS, "✓ ")
        for llm in detected:
            print(f"  • {llm}")
    else:
        styled_print("No platforms detected", Style.WARNING, "⚠ ")

    print_header("Step 3: Configuration")

    mode = prompt_choice(
        "How to proceed?",
        [
            "Quick start (defaults)",
            "Custom configuration",
            "Exit"
        ]
    )

    if mode == 3:
        styled_print("Exiting. Rerun with: python3 docker_wizard.py", Style.INFO, "ℹ ")
        sys.exit(0)

    env_vars = {}
    use_ollama = False

    if mode == 2:
        env_vars = configure_environment(detected)
        use_ollama = env_vars.get("OLLAMA_IN_DOCKER") == "true"
        if use_ollama:
            enable_ollama_service()
    else:
        styled_print("Using defaults (Ollama on host)", Style.INFO, "ℹ ")

    if prompt_yes_no("\nLaunch Text Loom now?"):
        launch_containers(use_ollama)
    else:
        styled_print("Start manually: docker-compose up -d", Style.INFO, "ℹ ")

    styled_print("\nHelp: https://github.com/kleer001/Text_Loom", Style.INFO, "ℹ ")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        styled_print("\n\nCancelled by user", Style.WARNING, "⚠ ")
        sys.exit(0)
    except Exception as e:
        styled_print(f"Unexpected error: {e}", Style.ERROR, "✗ ")
        sys.exit(1)
