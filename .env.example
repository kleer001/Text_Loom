# Text Loom Docker Environment Configuration
# Copy this file to .env and fill in your API keys

# =============================================================================
# LLM Configuration
# =============================================================================

# Active LLM platform (see src/core/settings.cfg for available options)
# Options: Ollama, LM Studio, GPT4All, LocalAI, llama.cpp, oobabooga,
#          ChatGPT, Perplexity, Claude, Gemini
ACTIVE_LLM=Ollama

# Default model to use
LLM_MODEL=llama3:latest

# =============================================================================
# Cloud LLM API Keys
# =============================================================================

# OpenAI API Key (for ChatGPT)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Anthropic API Key (for Claude)
# Get your key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# Perplexity API Key
# Get your key from: https://www.perplexity.ai/settings/api
PERPLEXITY_API_KEY=

# Google API Key (for Gemini)
# Get your key from: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=

# =============================================================================
# Local LLM Configuration
# =============================================================================

# If using local LLMs on host machine (Ollama, LM Studio, etc.):
# - Docker needs to access host services
# - Use 'host.docker.internal' instead of 'localhost' in settings.cfg
# - Or uncomment network_mode: host in docker-compose.yml

# If using Ollama in Docker (uncomment ollama service in docker-compose.yml):
# - Set url to http://ollama:11434 in settings.cfg

# =============================================================================
# Application Settings
# =============================================================================

# Python settings
PYTHONUNBUFFERED=1
